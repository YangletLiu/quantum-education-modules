{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b554e55-6654-4815-8a68-66511847deaf",
   "metadata": {},
   "source": [
    "Using reinforcement learning to optimize decision-making strategies for quantum circuit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9459a-5f98-42a6-b497-a81a38f07896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.circuit.library import HGate, CXGate, SGate, TGate, XGate, YGate, ZGate, CRZGate, TdgGate, UnitaryGate\n",
    "from qiskit.quantum_info import Operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e528a",
   "metadata": {},
   "source": [
    "# **Define Matrices and Operators**\n",
    "\n",
    "This section defines key quantum operators, unitary transformations, and quantum circuits for Bell states, GHZ states, and textbook examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c38e94-fa02-4dc2-95f7-d70df2775aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic Quantum gates\n",
    "\n",
    "H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "\n",
    "# Define matrices and operators\n",
    "swap_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "CNOT = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Bell state unitary\n",
    "bell_state_unitary = Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "phi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_plus = Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "\n",
    "# CZ matrix\n",
    "cz_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, -1]\n",
    "])\n",
    "\n",
    "# GHZ Circuit (3 qubits)\n",
    "ghz_circuit = QuantumCircuit(3)\n",
    "ghz_circuit.h(0)\n",
    "ghz_circuit.cx(0, 1)\n",
    "ghz_circuit.cx(1, 2)\n",
    "ghz_circuit = Operator(ghz_circuit)\n",
    "\n",
    "# Textbook circuits\n",
    "# page 200\n",
    "text_circuit1 = QuantumCircuit(3)\n",
    "text_circuit1.cx(0,1)\n",
    "text_circuit1.cx(1,2)\n",
    "text_circuit1.h(0)\n",
    "text_circuit1.h(1)\n",
    "text_circuit1.h(2)\n",
    "text_circuit1 = Operator(text_circuit1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82345c3-9bea-4c05-a858-e139d1584dc9",
   "metadata": {},
   "source": [
    "You can make your own circuit here and modify and test the effect of Q learning reinforcement learning algorithm in designing the circuit in the subsequent code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623cd96d-6c9f-46d6-b7cd-9dc9da921114",
   "metadata": {},
   "source": [
    "# **How to Create Your Own Quantum Circuit**\n",
    "\n",
    "Quantum computing circuits are designed using `Qiskit`, a quantum computing framework in Python. Below is a step-by-step guide on how to create your own quantum circuit.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Import Necessary Libraries**\n",
    "Before creating a quantum circuit, you need to import the required libraries.\n",
    "\n",
    "```python\n",
    "from qiskit import QuantumCircuit, Aer, transpile, assemble, execute\n",
    "from qiskit.quantum_info import Operator\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Create a Quantum Circuit**\n",
    "You can create a quantum circuit using `QuantumCircuit`. The number of qubits is specified as an argument.\n",
    "\n",
    "```python\n",
    "qc = QuantumCircuit(2)  # Create a quantum circuit with 2 qubits\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Apply Quantum Gates**\n",
    "Quantum gates manipulate qubits in different ways. Some commonly used quantum gates include:\n",
    "\n",
    "- **Hadamard Gate (H)**: Creates a superposition state.\n",
    "- **CNOT Gate (CX)**: Entangles two qubits.\n",
    "- **Pauli Gates (X, Y, Z)**: Represent basic quantum operations.\n",
    "\n",
    "Example of applying gates:\n",
    "\n",
    "```python\n",
    "qc.h(0)        # Apply Hadamard gate to qubit 0\n",
    "qc.cx(0, 1)    # Apply CNOT gate with qubit 0 as control and qubit 1 as target\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Convert the Circuit to a Matrix**\n",
    "To obtain the matrix representation of a circuit, use the `Operator` class:\n",
    "\n",
    "```python\n",
    "unitary_operator = Operator(qc)\n",
    "print(unitary_operator.data)  # Print the corresponding unitary matrix\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Check If a Matrix is Unitary**\n",
    "You make create a np.array to have your own Quantum Circuit.\n",
    "But have to make sure that it is a Unitary.\n",
    "\n",
    "\n",
    "### **Function to Check If a Matrix is Unitary**\n",
    "```python\n",
    "def is_unitary(matrix):\n",
    "    \"\"\"Check if a given np.array is a unitary matrix.\"\"\"\n",
    "    identity = np.eye(matrix.shape[0])  # Create an identity matrix of the same size\n",
    "    conjugate_transpose = np.conjugate(matrix).T  # Compute conjugate transpose\n",
    "    return np.allclose(identity, conjugate_transpose @ matrix) and np.allclose(identity, matrix @ conjugate_transpose)\n",
    "\n",
    "# Example matrices\n",
    "matrix1 = np.array([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "])  # Unitary matrix (Z gate)\n",
    "\n",
    "matrix2 = np.array([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])  # Not a unitary matrix\n",
    "\n",
    "print(is_unitary(matrix1))  # True\n",
    "print(is_unitary(matrix2))  # False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Test If Quantum Circuit Matrices Are Unitary**\n",
    "You can test whether predefined matrices (e.g., `swap_matrix`, `CNOT`, `cz_matrix`) are unitary:\n",
    "\n",
    "```python\n",
    "print(is_unitary(swap_matrix))  # Should return True\n",
    "print(is_unitary(CNOT))  # Should return True\n",
    "print(is_unitary(cz_matrix))  # Should return True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0e69f-a1f3-4dd4-8a51-9dcfc3692eb0",
   "metadata": {},
   "source": [
    "Hash function for Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4b55a-da5e-40a9-82c1-de906bf5c614",
   "metadata": {},
   "source": [
    "# **Matrix Hashing and Unique ID Assignment**\n",
    "\n",
    "This script provides a mechanism to assign unique IDs to matrices by hashing them into a dictionary. The purpose of this approach is to efficiently track and identify matrices without redundant storage.\n",
    "\n",
    "## **How It Works**\n",
    "1. **Matrix Hashing:**  \n",
    "   - The function `matrix_to_hash(matrix)` converts a given matrix into a hashable tuple format.  \n",
    "   - This ensures that matrices can be used as dictionary keys.\n",
    "\n",
    "2. **Unique ID Assignment:**  \n",
    "   - The function `get_matrix_id(matrix)` checks if a given matrix has been previously encountered.  \n",
    "   - If the matrix is new, it is assigned a unique ID and stored in `matrix_dict`.  \n",
    "   - If the matrix already exists in the dictionary, its previously assigned ID is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebaf3c1-400f-4000-b6b3-3c1560588c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store unique matrix hashes and their corresponding IDs\n",
    "matrix_dict = {}\n",
    "counter = 0 \n",
    "\n",
    "def matrix_to_hash(matrix):\n",
    "    \"\"\"\n",
    "    Convert a matrix to a hashable tuple format.\n",
    "    \"\"\"\n",
    "    matrix_array = np.asarray(matrix) \n",
    "    return tuple(tuple(row) for row in matrix_array)\n",
    "\n",
    "def get_matrix_id(matrix):\n",
    "    \"\"\"\n",
    "    Assign a unique ID to a matrix if it has not been encountered before.\n",
    "    \"\"\"\n",
    "    global counter\n",
    "    matrix_hash = matrix_to_hash(matrix)\n",
    "    \n",
    "    if matrix_hash not in matrix_dict:\n",
    "        matrix_dict[matrix_hash] = counter\n",
    "        counter += 1  \n",
    "    \n",
    "    return matrix_dict[matrix_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc79e19-67ca-411e-895d-49eaadef258e",
   "metadata": {},
   "source": [
    "# **1:Quantum Environment for Reinforcement Learning**\n",
    "\n",
    "This class is an implementation of a quantum environment using `OpenAI Gym` and `Qiskit`. The environment is designed for reinforcement learning (RL) tasks, where the goal is to apply quantum operations to match a target unitary transformation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "The environment simulates a **two-qubit quantum circuit**, where an agent applies quantum gates to reach a target unitary matrix. The circuit's state evolves as actions (quantum gates) are applied, and a reward function evaluates how close the resulting unitary matrix is to the target.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012e89e-ce2c-4990-b0dd-3e6c293573cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(QuantumEnv, self).__init__()\n",
    "        \n",
    "        # Set the number of qubits\n",
    "        self.num_qubits = 2\n",
    "        # Initialize the quantum circuit\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)\n",
    "        # Set the target unitary matrix (can be changed to bell_state, cz, swap, iswap)\n",
    "        self.target_unitary = iswap_matrix  \n",
    "        \n",
    "        # Define the action space (6 possible actions)\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        # Define the observation space (100 possible state hashes)\n",
    "        self.observation_space = spaces.Discrete(100)\n",
    "        \n",
    "        # Mapping of state indices\n",
    "        self.state_to_index = {}\n",
    "        self.index_to_state = []\n",
    "\n",
    "    def _hash_circuit(self, circuit: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Compute a hash value for the given quantum circuit.\n",
    "        \"\"\"\n",
    "        matrix = Operator(circuit)  # Get the unitary matrix of the circuit\n",
    "        return get_matrix_id(matrix) % 100  # Compute hash value within 100\n",
    "\n",
    "    def get_state_index(self, state: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of a state; if it is a new state, add it to the index mapping.\n",
    "        \"\"\"\n",
    "        state_hash = self._hash_circuit(state)\n",
    "        if state_hash not in self.state_to_index:\n",
    "            index = len(self.state_to_index)\n",
    "            self.state_to_index[state_hash] = index\n",
    "            self.index_to_state.append(state)\n",
    "        return self.state_to_index[state_hash]\n",
    "\n",
    "    def get_state_from_index(self, index: int) -> QuantumCircuit:\n",
    "        \"\"\"\n",
    "        Retrieve the quantum circuit state based on the index.\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.index_to_state):\n",
    "            return self.index_to_state[index]\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state index.\n",
    "        \"\"\"\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)  # Reinitialize the circuit\n",
    "        return self.get_state_index(self.circuit)\n",
    "\n",
    "    def step(self, action, qubits):\n",
    "        \"\"\"\n",
    "        Execute an action, update the environment state, and compute the reward.\n",
    "        \"\"\"\n",
    "        self.circuit.append(action, qubits)  # Append the action to the circuit\n",
    "        state_index = self.get_state_index(self.circuit)  # Get the new state index\n",
    "        reward, done = self._reward(self.target_unitary)  # Compute the reward\n",
    "        return state_index, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())\n",
    "\n",
    "    def _reward(self, target_unitary):\n",
    "        \"\"\"\n",
    "        Compute the fidelity between the circuit and the target unitary matrix and return the reward.\n",
    "        \"\"\"\n",
    "        simulator = Aer.get_backend('unitary_simulator')  # Get the unitary simulator\n",
    "        result = simulator.run(transpile(self.circuit, simulator)).result()\n",
    "        unitary = result.get_unitary(self.circuit)  # Get the unitary matrix of the current circuit\n",
    "        \n",
    "        # Compute the fidelity of the quantum state\n",
    "        unitary_array = np.asarray(unitary)\n",
    "        target_unitary_array = np.asarray(target_unitary)\n",
    "        fidelity = np.abs(np.trace(unitary_array.conj().T @ target_unitary_array)) / (2 ** self.num_qubits)\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        if fidelity > 0.99:\n",
    "            done = True  # Task completed\n",
    "            reward += 100  # Assign high reward\n",
    "            self.render()  # Display the final circuit\n",
    "        return reward, done\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b0861-b656-4d25-a50e-5f1a780ecdac",
   "metadata": {},
   "source": [
    "# **2:Q-learning Agent for Quantum Reinforcement Learning**\n",
    "\n",
    "This class is a **Q-learning agent** designed for reinforcement learning in a quantum environment. The agent learns how to construct quantum circuits by selecting quantum gates to maximize a reward function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993060c-f3a6-4fcf-a3f1-f4ef1f0207ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, alpha, gamma, epsilon, decay_rate, epsilon_min):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent with given parameters.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.decay_rate = decay_rate  # Decay rate for epsilon\n",
    "        self.epsilon_min = epsilon_min  # Minimum value of epsilon\n",
    "        self.q_table = np.zeros((state_size, action_size))  # Initialize Q-table with zeros\n",
    "    \n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.action_size)  # Random action (exploration)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state_index])  # Best action (exploitation)\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "\n",
    "    def choose_actionNoE(self, state_index):\n",
    "        \"\"\"\n",
    "        Select the best action based on the current Q-table without exploration.\n",
    "        \"\"\"\n",
    "        action = np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "    \n",
    "    def update_q_table(self, state_index, action, reward, next_state_index):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning formula.\n",
    "        \"\"\"\n",
    "        self.q_table[state_index, action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.q_table[next_state_index]) - self.q_table[state_index, action]\n",
    "        )\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        \"\"\"\n",
    "        Reduce epsilon value over time to shift from exploration to exploitation.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.decay_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae60713",
   "metadata": {},
   "source": [
    "# **3:Train the Agent**\n",
    "\n",
    "The `train_agent` function is responsible for training a reinforcement learning (RL) agent to optimize decision-making in a given environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222e852-009a-43f8-b3c4-6750937316cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "def train_agent(agent, environment, episodes, max_steps_per_episode):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment at the beginning of each episode\n",
    "        state_index = environment.reset()\n",
    "        episode_reward = 0  # Initialize the total reward for this episode\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action using the agent's policy\n",
    "            action, action_index = agent.choose_action(state_index)\n",
    "            \n",
    "            # Execute the chosen action and observe the outcome\n",
    "            next_state_index, reward, done = environment.step(action[0], action[1])\n",
    "            episode_reward += reward  # Accumulate the reward\n",
    "            \n",
    "            # Update the Q-table based on the agent's learning algorithm\n",
    "            agent.update_q_table(state_index, action_index, reward, next_state_index)\n",
    "            \n",
    "            # Update the current state\n",
    "            state_index = next_state_index\n",
    "            \n",
    "            # Check if the episode has reached a terminal state\n",
    "            if done:\n",
    "                print(\"Generated circuit:\")\n",
    "                environment.render()  # Render the environment to visualize the result\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "                break\n",
    "            \n",
    "            # Apply a penalty if the circuit exceeds the maximum number of allowed gates\n",
    "            if environment.circuit.size() > 4:\n",
    "                episode_reward -= 100  # Negative reward for exceeding the maximum gate limit\n",
    "                break\n",
    "        \n",
    "        # Print results every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "        \n",
    "        # Decay the exploration rate to encourage exploitation over time\n",
    "        agent.decay_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5085a",
   "metadata": {},
   "source": [
    "# **4:Test the Trained Agent Without Exploration**\n",
    "\n",
    "The `test_agent` function evaluates a trained reinforcement learning (RL) agent by running it in the environment **without exploration** (i.e., the agent strictly follows the learned policy).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf932ca-d86a-4ac8-97af-b69b3204b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent without exploration\n",
    "def test_agent(agent, environment, episodes, max_steps_per_episode):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment\n",
    "        environment.reset()\n",
    "        state_index = environment.reset()\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action based purely on learned policy (no exploration)\n",
    "            action, action_index = agent.choose_actionNoE(state_index)\n",
    "            \n",
    "            # Execute the chosen action and observe the outcome\n",
    "            next_state_index, reward, done = environment.step(action[0], action[1])\n",
    "            \n",
    "            # Update the current state\n",
    "            state_index = next_state_index\n",
    "            \n",
    "            # Check if the episode has reached a terminal state\n",
    "            if done:\n",
    "                global holder\n",
    "                holder += 1  # Increment success counter\n",
    "                break\n",
    "        \n",
    "        # Render the environment to visualize the test result\n",
    "        environment.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324dca3",
   "metadata": {},
   "source": [
    "\n",
    "# **5:Main Function**\n",
    "\n",
    "The `__main__` block initializes and trains a reinforcement learning agent multiple times, followed by testing its performance. It also calculates and prints the average success rate.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fda2df-a893-43cf-8b02-9997d3548e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global holder\n",
    "holder = 0  # Initialize success counter\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Run multiple training and testing iterations\n",
    "    for i in range(20):\n",
    "        # Initialize the environment and agent for each iteration\n",
    "        environment = QuantumEnv()\n",
    "        agent = QLearningAgent(state_size=100, action_size=6, alpha=0.1, gamma=0.95, epsilon=1, decay_rate=0.99, epsilon_min=0.05)\n",
    "        \n",
    "        # Train the agent\n",
    "        train_agent(agent, environment, episodes=100, max_steps_per_episode=5)\n",
    "        \n",
    "        # Test the trained agent\n",
    "        print(\"Test Result\")\n",
    "        test_agent(agent, environment, episodes=1, max_steps_per_episode=5)\n",
    "    \n",
    "    # Print the average success rate over 20 iterations\n",
    "    print(holder / 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
